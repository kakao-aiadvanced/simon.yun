{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dp8QNI-b_6dV"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tavily example\n",
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key='')\n",
        "\n",
        "response = tavily.search(query=\"Where does Messi play right now?\", max_results=3)\n",
        "context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
        "\n",
        "# You can easily get search result context based on any max tokens straight into your RAG.\n",
        "# The response is a string of the context within the max_token limit.\n",
        "\n",
        "response_context = tavily.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500)\n",
        "\n",
        "# You can also get a simple answer to a question including relevant sources all with a simple function call:\n",
        "# You can use it for baseline\n",
        "response_qna = tavily.qna_search(query=\"Where does Messi play right now?\")"
      ],
      "metadata": {
        "id": "bGSx9-SNAaPP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)"
      ],
      "metadata": {
        "id": "MuBENbA3C09E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Index\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_htEIf5I4W5u",
        "outputId": "401e99dd-6f21-414f-826c-a23466dc036c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# State\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: List[str]\n",
        "    is_generated : bool\n",
        "    is_relevance_checked : bool\n",
        "\n",
        "# Nodes\n",
        "\n",
        "## Docs Retrieval\n",
        "def DocsRetrieval(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents from vectorstore\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    print(question)\n",
        "    print(documents)\n",
        "    return {\"documents\": documents, \"question\": question} # 4개 docs가 반환됨\n",
        "\n",
        "## Relevance Checker\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def RelevanceChecker(state) :\n",
        "  \"\"\"\n",
        "  Determines whether the retrieved documents are relevant to the question\n",
        "  If any document is not relevant, we will set a flag to run web search\n",
        "\n",
        "  Args:\n",
        "      state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "      state (dict): Filtered out irrelevant documents and updated web_search state\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  system = \"\"\"You are a grader assessing relevance\n",
        "      of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "      grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "      Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "      Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
        "      \"\"\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "          (\"human\", \"question: {question}\\n\\n document: {document} \"),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  retrieval_grader = prompt | llm | JsonOutputParser()\n",
        "  if state[\"is_relevance_checked\"] :\n",
        "    print(\"RelevanceChecker : 2번째 relevance_check 진행\")\n",
        "    # web Search 결과도 이상할 때 종료하는 부분\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "      score = retrieval_grader.invoke(\n",
        "          {\"question\": question, \"document\": d.page_content}\n",
        "      )\n",
        "      grade = score[\"score\"]\n",
        "      # Document relevant\n",
        "      if grade.lower() == \"yes\":\n",
        "        print(\"RelevanceChecker : ===관련있===\")\n",
        "        filtered_docs.append(d)\n",
        "      # Document not relevant\n",
        "      else:\n",
        "        print(\"RelevanceChecker : ===관련없===\")\n",
        "        # We do not include the document in filtered_docs\n",
        "        # We set a flag to indicate that we want to run web search\n",
        "        continue\n",
        "    if len(filtered_docs) > 0 :\n",
        "      return {\"documents\": filtered_docs, \"question\": question, \"web_search\": \"no\", \"is_relevance_checked\" : True}\n",
        "    else :\n",
        "      print(\"RelevanceChecker : shit\")\n",
        "      return {\"documents\": filtered_docs, \"question\": question, \"web_search\": \"done\", \"is_relevance_checked\" : True}\n",
        "  else :\n",
        "    print(\"RelevanceChecker : 1번째 relevance_check 진행\")\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "      score = retrieval_grader.invoke(\n",
        "          {\"question\": question, \"document\": d.page_content}\n",
        "      )\n",
        "      grade = score[\"score\"]\n",
        "      # Document relevant\n",
        "      if grade.lower() == \"yes\":\n",
        "        print(\"RelevanceChecker : ===관련있===\")\n",
        "        filtered_docs.append(d)\n",
        "      # Document not relevant\n",
        "      else:\n",
        "        print(\"RelevanceChecker : ===관련없===\")\n",
        "        # We do not include the document in filtered_docs\n",
        "        # We set a flag to indicate that we want to run web search\n",
        "        web_search = \"Yes\"\n",
        "        continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search, \"is_relevance_checked\" : True}\n",
        "\n",
        "## Generate Answer\n",
        "\n",
        "def generate(state):\n",
        "  \"\"\"\n",
        "  Generate answer using RAG on retrieved documents\n",
        "\n",
        "  Args:\n",
        "      state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "      state (dict): New key added to state, generation, that contains LLM generation\n",
        "  \"\"\"\n",
        "  system = \"\"\"You are an assistant for question-answering tasks.\n",
        "      Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "      Use three sentences maximum and keep the answer concise\"\"\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "          (\"human\", \"question: {question}\\n\\n context: {context} \"),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # Chain\n",
        "  rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "  print(\"generate : generate 시작\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  # RAG generation\n",
        "  generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "\n",
        "  is_relevance_checked = state[\"is_relevance_checked\"]\n",
        "  return {\"documents\": documents, \"question\": question, \"generation\": generation,\n",
        "          \"is_relevance_checked\" : is_relevance_checked,\n",
        "          \"is_generated\" : True\n",
        "          }\n",
        "\n",
        "\n",
        "## Search Tavily\n",
        "def web_search(state):\n",
        "  \"\"\"\n",
        "  Web search based based on the question\n",
        "\n",
        "  Args:\n",
        "      state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "      state (dict): Appended web results to documents\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"web_search : web_search 시작\")\n",
        "  print(state)\n",
        "  question = state[\"question\"]\n",
        "  documents = None\n",
        "  if \"documents\" in state:\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "  # Web search\n",
        "  docs = tavily.search(query=question)['results']\n",
        "\n",
        "  web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "  web_results = Document(page_content=web_results)\n",
        "  if documents is not None:\n",
        "      documents.append(web_results)\n",
        "  else:\n",
        "      documents = [web_results]\n",
        "  return {\"documents\": documents, \"question\": question,\n",
        "          \"is_relevance_checked\" : state[\"is_relevance_checked\"]\n",
        "          }"
      ],
      "metadata": {
        "id": "lRbR-rtaC9Jw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# edge node\n",
        "def generate_or_web_search(state):\n",
        "  \"\"\"\n",
        "  Route question to web search or generate.\n",
        "\n",
        "  Args:\n",
        "      state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "      str: Next node to call\n",
        "  \"\"\"\n",
        "  if state[\"web_search\"] == 'done' : return 'done'\n",
        "  print(\"generate_or_web_search : web? or generate?\")\n",
        "  print(state)\n",
        "  question = state[\"question\"]\n",
        "  print(question)\n",
        "\n",
        "  # source = question_router.invoke({\"question\": question})\n",
        "  # print(source)\n",
        "\n",
        "  if state[\"web_search\"].lower() == \"no\":\n",
        "      print(\"generate 하기로 결정\")\n",
        "      return \"generate\"\n",
        "  elif state[\"web_search\"].lower() == \"yes\":\n",
        "      print(\"web_search 하기로 결정\")\n",
        "      return \"websearch\"\n",
        "  else :\n",
        "    print(\"shit\")\n",
        "    return \"done\"\n",
        "\n",
        "def re_generate_or_answer(state) :\n",
        "  ### Hallucination Grader\n",
        "\n",
        "  system = \"\"\"You are a grader assessing whether\n",
        "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
        "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation.\"\"\"\n",
        "\n",
        "  docs = state[\"documents\"]\n",
        "  generation = state[\"generation\"]\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"documents: {documents}\\n\\n answer: {generation} \"),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  hallucination_grader = prompt | llm | JsonOutputParser()\n",
        "  rst = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
        "\n",
        "  if rst[\"score\"].lower() =='yes' :\n",
        "    return 'answer'\n",
        "  elif rst['score'].lower() == 'no' :\n",
        "    if state['is_generated'] :\n",
        "      # 할루시네이션 2번 실패했을 때 종료 처리 채우기\n",
        "      return \"Done\"\n",
        "    return 'regenerate'"
      ],
      "metadata": {
        "id": "GC6m7IuGAa-6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", DocsRetrieval)  # retrieve\n",
        "workflow.add_node(\"relevance_check\", RelevanceChecker)  # RelevanceChecker\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"websearch\", web_search)  # web search\n",
        "\n",
        "# make graph\n",
        "workflow.set_entry_point(\"retrieve\") # 1\n",
        "workflow.add_edge(\"retrieve\", \"relevance_check\") # 2\n",
        "# 3\n",
        "## 3-1 : relevance 통과 후 generate 보내는 부분\n",
        "workflow.add_conditional_edges(\n",
        "    \"relevance_check\",\n",
        "    generate_or_web_search,\n",
        "    {\n",
        "        \"websearch\": \"websearch\",\n",
        "        \"generate\": \"generate\",\n",
        "        \"done\" : END\n",
        "    },\n",
        ")\n",
        "## 3-2 : web_search 후 relevance check 다시 하는 부분\n",
        "workflow.add_edge(\"websearch\", \"relevance_check\") # 2\n",
        "\n",
        "# 4\n",
        "## 분기 노드\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    re_generate_or_answer,\n",
        "    {\n",
        "        \"regenerate\": \"generate\",\n",
        "        \"answer\": END, # 답변 출력\n",
        "        \"Done\" : END # failed\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9dwfFbI6fVo",
        "outputId": "7bf715e8-0f41-4293-b323-0b83e268cb71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79976301c7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\"question\": \"What is prompt?\",\n",
        "         \"is_regenerate\" : False,\n",
        "         \"is_relevance_checked\" : False,\n",
        "         }\n",
        "\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "# Test\n",
        "\n",
        "inputs = {\"question\": \"축구선수 메시는 현재 무슨 팀에 있어?\",\n",
        "         \"is_regenerate\" : False,\n",
        "         \"is_relevance_checked\" : False,\n",
        "         }\n",
        "for output in app.stream(inputs):\n",
        "  for key, value in output.items():\n",
        "      pprint(f\"Finished running: {key}:\")\n",
        "try :\n",
        "  pprint(value[\"generation\"])\n",
        "except :\n",
        "  pprint(\"failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM_gWj0A9D01",
        "outputId": "1076e455-bb05-4f91-d0df-552e914b3b74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "축구선수 메시는 현재 무슨 팀에 있어?\n",
            "[Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'), Document(metadata={'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='The above token search method can be augmented with beam search. When looking for the optimal token embedding $\\\\mathbf{e}’_i$, we can pick top-$k$ candidates instead of a single one, searching from left to right and score each beam by $\\\\mathcal{L}_\\\\text{adv}$ on the current data batch.'), Document(metadata={'title': \"Prompt Engineering | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'language': 'en', 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.'}, page_content='$$\\n  \\\\begin{aligned}\\n  e(c) &= \\\\langle\\\\texttt{API}\\\\rangle a_c(i_c) \\\\langle\\\\texttt{/API}\\\\rangle \\\\\\\\\\n  e(c, r) &= \\\\langle\\\\texttt{API}\\\\rangle a_c(i_c) \\\\to r \\\\langle\\\\texttt{/API}\\\\rangle\\n  \\\\end{aligned}\\n  $$\\n  \\n\\n\\nSample API calls based on the probabilities $p_\\\\text{LM}(\\\\langle\\\\texttt{API}\\\\rangle \\\\mid \\\\text{prompt}(\\\\mathbf{x}), \\\\mathbf{x}_{1:i})$ and select top $k$ candidate positions for doing API calls at position $i$ if the probability is larger than a threshold.'), Document(metadata={'language': 'en', 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\"}, page_content='Casper et al. (2023) set up a human-in-the-loop red teaming process. The main difference from Perez et al. (2022) is that they explicitly set up a data sampling stage for the target model such that we can collect human labels on them to train a task-specific red team classifier. There are three steps:')]\n",
            "'Finished running: retrieve:'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "RelevanceChecker : 1번째 relevance_check 진행\n",
            "RelevanceChecker : ===관련없===\n",
            "RelevanceChecker : ===관련없===\n",
            "RelevanceChecker : ===관련없===\n",
            "RelevanceChecker : ===관련없===\n",
            "generate_or_web_search : web? or generate?\n",
            "{'documents': [], 'question': '축구선수 메시는 현재 무슨 팀에 있어?', 'web_search': 'Yes', 'is_relevance_checked': True}\n",
            "축구선수 메시는 현재 무슨 팀에 있어?\n",
            "web_search 하기로 결정\n",
            "'Finished running: relevance_check:'\n",
            "web_search : web_search 시작\n",
            "{'question': '축구선수 메시는 현재 무슨 팀에 있어?', 'web_search': 'Yes', 'documents': [], 'is_relevance_checked': True}\n",
            "'Finished running: websearch:'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "RelevanceChecker : 2번째 relevance_check 진행\n",
            "RelevanceChecker : ===관련있===\n",
            "generate_or_web_search : web? or generate?\n",
            "{'documents': [Document(metadata={}, page_content='월드컵 우승 당시만 해도 프랑스 프로축구 리그에서 뛰었던 메시는 2023년 7월 축구의 인기가 유럽이나 중남미보다 못한 미국 리그로 옮겼다. 플로리다주(州) 마이애미가 연고지인 인터 마이애미 cf가 현재 메시의 소속 팀이다.\\n현재 메시는 클럽 커리어는 사실상 마감한 상태다. 인터 마이애미에서는 이른바 \\'행복 축구\\'를 즐기며 큰 부담없이 경기를 치르고 있다. ... 선택한 것은 가족 문제도 있으나 그가 2026 북중미 월드컵을 염두에 두고 있을 가능성이 있어보인다. 미국 국가 자체로도\\n한눈에 보는 오늘 : 해외축구 - 뉴스 : [인터풋볼]주대은기자=리오넬 메시가 바르셀로나 복귀에 대한 이야기를 꺼냈다.스포츠 매체 \\'트리뷰나\\'는 18일(한국시간) \"메시는 파리 생제르맹(PSG)를 떠나면서 바르셀로나 복귀를 원했지만, 이적이 실현되지 않았다\"라고 보도했다.\\'트리뷰나\\'에 따르면\\n한눈에 보는 오늘 : 해외축구 - 뉴스 : [스포티비뉴스=박대성 기자] 바르셀로나가 리오넬 메시(37)의 재영입을 준비하고 있다. 현지에서도 \\\\\\'충격\\\\\\'이라는 단어로 메시의 바르셀로나 복귀 가능성을 보도했다. 19일(한국시간) 아르헨티나 방송 매체 \\\\\\'tnt스포츠\\\\\\'에 따르면 바르셀로나는 20\\n유스 시절부터 약 20여 년 동안 바르셀로나에서 뛴 메시는 통산 778경기 672골 303도움이라는 전무후무한 기록을 남겼다. 이 기간, 메시는 스페인 라 리가 10회, 유럽축구연맹(UEFA) 챔피언스리그 4회 등 수많은 우승 트로피를 들어 올렸고, 바르셀로나 그 자체로')], 'question': '축구선수 메시는 현재 무슨 팀에 있어?', 'web_search': 'no', 'is_relevance_checked': True}\n",
            "축구선수 메시는 현재 무슨 팀에 있어?\n",
            "generate 하기로 결정\n",
            "'Finished running: relevance_check:'\n",
            "generate : generate 시작\n",
            "'Finished running: generate:'\n",
            "(\"리오넬 메시는 현재 인터 마이애미 CF에 소속되어 있습니다. 그는 2023년 7월에 미국 리그로 이적하였고, 현재는 '행복 축구'를 \"\n",
            " '즐기고 있습니다.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCCT5FunYDHY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxpoQ9vjYJ8B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "becpmVHxao11"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}